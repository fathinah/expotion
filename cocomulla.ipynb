{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load....musicgen bk\n",
      "lm_bk, here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIPPING /l/users/fathinah.izzati/coco-mulla-repo/demo/output/chord-only happening with proba (a bit of clipping is okay): 0.00012656250328291208 maximum scale:  1.2986297607421875\n",
      "CLIPPING /l/users/fathinah.izzati/coco-mulla-repo/demo/output/chord-drums happening with proba (a bit of clipping is okay): 7.812499825377017e-05 maximum scale:  1.1229928731918335\n",
      "CLIPPING /l/users/fathinah.izzati/coco-mulla-repo/demo/output/chord-midi happening with proba (a bit of clipping is okay): 3.12499992105586e-06 maximum scale:  1.0149791240692139\n",
      "CLIPPING /l/users/fathinah.izzati/coco-mulla-repo/demo/output/chord-drums-midi happening with proba (a bit of clipping is okay): 4.687500222644303e-06 maximum scale:  1.1103787422180176\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import librosa\n",
    "\n",
    "from coco_mulla.models import CoCoMulla\n",
    "from coco_mulla.utilities import *\n",
    "from coco_mulla.utilities.encodec_utils import extract_rvq, save_rvq\n",
    "from coco_mulla.utilities.symbolic_utils import process_midi, process_chord\n",
    "\n",
    "from coco_mulla.utilities.sep_utils import separate\n",
    "from config import TrainCfg\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "def generate(model_path, batch):\n",
    "    model = CoCoMulla(TrainCfg.sample_sec,\n",
    "                      num_layers=args.num_layers,\n",
    "                      latent_dim=args.latent_dim).to(device)\n",
    "    model.load_weights(model_path)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model(**batch)\n",
    "\n",
    "    return gen_tokens\n",
    "\n",
    "\n",
    "def generate_mask(xlen):\n",
    "    names = [\"chord-only\", \"chord-drums\", \"chord-midi\", \"chord-drums-midi\"]\n",
    "    mask = torch.zeros([4, 2, xlen]).to(device)\n",
    "    mask[1, 1] = 1\n",
    "    mask[2, 0] = 1\n",
    "    mask[3] += 1\n",
    "    return mask, names\n",
    "\n",
    "\n",
    "def load_data(audio_path, chord_path, midi_path, offset):\n",
    "    sr = TrainCfg.sample_rate\n",
    "    res = TrainCfg.frame_res\n",
    "    sample_sec = TrainCfg.sample_sec\n",
    "\n",
    "    wav, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
    "    wav = np2torch(wav).to(device)[None, None, ...]\n",
    "    wavs = separate(wav, sr)\n",
    "    drums_rvq = extract_rvq(wavs[\"drums\"], sr=sr)\n",
    "    chord, _ = process_chord(chord_path)\n",
    "    flatten_midi_path = midi_path + \".piano.mid\"\n",
    "    midi, _ = process_midi(midi_path)\n",
    "\n",
    "\n",
    "\n",
    "    chord = crop(chord[None, ...], \"chord\", sample_sec, res)\n",
    "    pad_chord = chord.sum(-1, keepdims=True) == 0\n",
    "    chord = np.concatenate([chord, pad_chord], -1)\n",
    "\n",
    "    midi = crop(midi[None, ...], \"midi\", sample_sec, res,offset=offset)\n",
    "    drums_rvq = crop(drums_rvq[None, ...], \"drums_rvq\", sample_sec, res, offset=offset)\n",
    "\n",
    "    chord = torch.from_numpy(chord).to(device).float()\n",
    "    midi = torch.from_numpy(midi).to(device).float()\n",
    "    drums_rvq = drums_rvq.to(device).long()\n",
    "\n",
    "    return drums_rvq, midi, chord\n",
    "\n",
    "\n",
    "def crop(x, mode, sample_sec, res, offset=0):\n",
    "    xlen = x.shape[1] if mode == \"chord\" or mode == \"midi\" else x.shape[-1]\n",
    "    sample_len = int(sample_sec * res) + 1\n",
    "    if xlen < sample_len:\n",
    "        if mode == \"chord\" or mode == \"midi\":\n",
    "            x = np.pad(x, ((0, 0), (0, sample_len - xlen), (0, 0)))\n",
    "        else:\n",
    "            x = F.pad(x, (0, sample_len - xlen), \"constant\", 0)\n",
    "        return x\n",
    "\n",
    "    st = offset * res\n",
    "    ed = int((offset + sample_sec) * res) + 1\n",
    "    if mode == \"chord\" or mode == \"midi\":\n",
    "        assert x.shape[1] > st\n",
    "        return x[:, st: ed]\n",
    "    assert x.shape[2] > ed\n",
    "    return x[:, :, st: ed]\n",
    "\n",
    "\n",
    "def save_pred(output_folder, tags, pred):\n",
    "    mkdir(output_folder)\n",
    "    output_list = [os.path.join(output_folder, tag) for tag in tags]\n",
    "    save_rvq(output_list=output_list, tokens=pred)\n",
    "\n",
    "\n",
    "def wrap_batch(drums_rvq, midi, chord, cond_mask, prompt):\n",
    "    num_samples = len(cond_mask)\n",
    "    midi = midi.repeat(num_samples, 1, 1)\n",
    "    chord = chord.repeat(num_samples, 1, 1)\n",
    "    drums_rvq = drums_rvq.repeat(num_samples, 1, 1)\n",
    "    prompt = [prompt] * num_samples\n",
    "    batch = {\n",
    "        \"seq\": None,\n",
    "        \"desc\": prompt,\n",
    "        \"chords\": chord,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"cond_mask\": cond_mask,\n",
    "        \"drums\": drums_rvq,\n",
    "        \"piano_roll\": midi,\n",
    "        \"mode\": \"inference\",\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def inference(args):\n",
    "    drums_rvq, midi, chord = load_data(audio_path=args.audio_path,\n",
    "                                       chord_path=args.chord_path,\n",
    "                                       midi_path=args.midi_path,\n",
    "                                       offset=args.offset)\n",
    "    cond_mask, names = generate_mask(drums_rvq.shape[-1])\n",
    "    batch = wrap_batch(drums_rvq, midi, chord, cond_mask, read_lst(args.prompt_path)[0])\n",
    "    pred = generate(model_path=args.model_path,\n",
    "                    batch=batch)\n",
    "    save_pred(output_folder=args.output_folder,\n",
    "              tags=names,\n",
    "              pred=pred)\n",
    "\n",
    "from types import SimpleNamespace\n",
    "args = {\n",
    "    \"num_layers\": 48,\n",
    "    \"latent_dim\": 12,\n",
    "    \"output_folder\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/output\",\n",
    "    \"model_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/diff_9_end.pth\",\n",
    "    \"audio_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.flac\",\n",
    "    \"prompt_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.prompt.txt\",\n",
    "    \"chord_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.flac.chord.lab\",\n",
    "    \"midi_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.mid.piano.mid\",\n",
    "    \"drums_path\": None,\n",
    "    \"offset\": 0\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "drums_rvq, midi, chord = load_data(audio_path=args.audio_path,\n",
    "                                    chord_path=args.chord_path,\n",
    "                                    midi_path=args.midi_path,\n",
    "                                    offset=args.offset)\n",
    "cond_mask, names = generate_mask(drums_rvq.shape[-1])\n",
    "batch = wrap_batch(drums_rvq, midi, chord, cond_mask, read_lst(args.prompt_path)[0])\n",
    "pred = inference(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import spawn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from coco_mulla.utilities.trainer_utils import Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from config import TrainCfg\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from coco_mulla.data_loader.dataset_sampler import Dataset, collate_fn\n",
    "from coco_mulla.models import CoCoMulla\n",
    "\n",
    "device = \"cuda\"\n",
    "N_GPUS = 2\n",
    "\n",
    "\n",
    "def _get_free_port():\n",
    "    import socketserver\n",
    "    with socketserver.TCPServer(('localhost', 0), None) as s:\n",
    "        return s.server_address[1]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset_split, sampling_strategy, sampling_prob):\n",
    "\n",
    "    file_lst = [\"data/text/musdb18_full.lst\",\n",
    "                \"data/text/closed_dataset_fm_full.lst\"]\n",
    "    splits = [\n",
    "        [1],\n",
    "        [0],\n",
    "        [0, 1],\n",
    "    ]\n",
    "    dataset = Dataset(\n",
    "        rid=0, # No distributed rank needed\n",
    "        path_lst=[dataset_split],\n",
    "        sampling_prob=sampling_prob,\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        cfg=TrainCfg)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=TrainCfg.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        # sampler=DistributedSampler(dataset),\n",
    "        pin_memory=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "def train_dist(replica_id, replica_count, port, model_dir, args):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = str(port)\n",
    "    torch.distributed.init_process_group('nccl', rank=replica_id, world_size=replica_count)\n",
    "    device = torch.device('cuda', replica_id)\n",
    "    torch.cuda.set_device(device)\n",
    "    model = CoCoMulla(TrainCfg.sample_sec, num_layers=args.num_layers, latent_dim=args.latent_dim).to(device)\n",
    "    model.set_training()\n",
    "    model = DDP(model, [replica_id])\n",
    "    dataset, dataloader = get_dataset(rid=replica_id, dataset_split=args.dataset,\n",
    "                                      sampling_strategy=args.sampling_strategy,\n",
    "                                      sampling_prob=[args.sampling_prob_a, args.sampling_prob_b])\n",
    "\n",
    "    train(replica_id, model, dataset, dataloader, device, model_dir,\n",
    "          args.learning_rate)\n",
    "\n",
    "\n",
    "def loss_fn(outputs, y):\n",
    "    prob = outputs.logits\n",
    "    mask = outputs.mask\n",
    "    prob = prob[mask]\n",
    "    y = y[mask]\n",
    "    prob = prob.view(-1, 2048)\n",
    "    return nn.CrossEntropyLoss()(prob, y)\n",
    "\n",
    "\n",
    "def train(model, dataset, dataloader, device, model_dir, learning_rate):\n",
    "    # optimizer and lr scheduler\n",
    "    num_steps = len(dataloader)\n",
    "    epochs = TrainCfg.epoch\n",
    "    rng = np.random.RandomState(569)\n",
    "    writer = SummaryWriter(model_dir, flush_secs=20)\n",
    "\n",
    "    trainer = Trainer(params=model.parameters(), lr=learning_rate, num_epochs=epochs, num_steps=num_steps)\n",
    "\n",
    "    model = model.to(device)\n",
    "    step = 0\n",
    "    for e in range(0, epochs):\n",
    "        mean_loss = 0\n",
    "        n_element = 0\n",
    "        model.train()\n",
    "\n",
    "        dl = tqdm(dataloader, desc=f\"Epoch {e}\")\n",
    "        r = rng.randint(0, 233333)\n",
    "        dataset.reset_random_seed(r, e)\n",
    "        for i, batch in enumerate(dl):\n",
    "            desc = batch[\"desc\"]\n",
    "            mix = batch[\"mix\"].to(device).long()\n",
    "            drums = batch[\"drums\"].to(device).long()\n",
    "            chords = batch[\"chords\"].to(device).float()\n",
    "            piano_roll = batch[\"piano_roll\"].to(device).float()\n",
    "            cond_mask = batch[\"cond_mask\"].to(device).long()\n",
    "\n",
    "            batch_1 = {\n",
    "                \"seq\": mix,\n",
    "                \"drums\": drums,\n",
    "                \"chords\": chords,\n",
    "                \"piano_roll\": piano_roll,\n",
    "                \"cond_mask\": cond_mask,\n",
    "                \"desc\": desc,\n",
    "\n",
    "            }\n",
    "            # with autocast:\n",
    "            outputs = model(**batch_1)\n",
    "            r_loss = loss_fn(outputs, mix.long())\n",
    "\n",
    "            grad_1, lr_1 = trainer.step(r_loss, model.parameters())\n",
    "\n",
    "            step += 1\n",
    "            n_element += 1\n",
    "            writer.add_scalar(\"r_loss\", r_loss.item(), step)\n",
    "            writer.add_scalar(\"grad_1\", grad_1, step)\n",
    "            writer.add_scalar(\"lr_1\", lr_1, step)\n",
    "\n",
    "            mean_loss += r_loss.item()\n",
    "\n",
    "        mean_loss = mean_loss / n_element\n",
    "        with torch.no_grad():\n",
    "            writer.add_scalar('train/mean_loss', mean_loss, step)\n",
    "            model.module.save_weights(os.path.join(model_dir, f\"diff_{e}_end.pth\"))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    experiment_folder = args.experiment_folder\n",
    "    experiment_name = args.experiment_name\n",
    "\n",
    "    if not os.path.exists(experiment_folder):\n",
    "        os.mkdir(experiment_folder)\n",
    "    model_dir = os.path.join(experiment_folder, experiment_name)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    world_size = N_GPUS\n",
    "    port = _get_free_port()\n",
    "    spawn(train_dist, args=(world_size, port, model_dir, args), nprocs=world_size, join=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /l/users/fathinah.izzati/coco-mulla-repo/train.lst\n",
      "[{'path': '/l/users/fathinah.izzati/coco-mulla-repo/demo/data/let_it_be', 'data': {'piano_roll': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])}}, {'path': '/l/users/fathinah.izzati/coco-mulla-repo/demo/data/let_it_be', 'data': {'piano_roll': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])}}, {'path': '/l/users/fathinah.izzati/coco-mulla-repo/demo/data/let_it_be', 'data': {'piano_roll': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])}}, {'path': '/l/users/fathinah.izzati/coco-mulla-repo/demo/data/let_it_be', 'data': {'piano_roll': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])}}] [[0, 0, 10], [0, 0, 20], [0, 0, 30], [0, 0, 40], [0, 0, 50], [0, 0, 60], [0, 0, 70], [0, 0, 80], [0, 1, 10], [0, 1, 20], [0, 1, 30], [0, 1, 40], [0, 1, 50], [0, 1, 60], [0, 1, 70], [0, 1, 80], [0, 2, 10], [0, 2, 20], [0, 2, 30], [0, 2, 40], [0, 2, 50], [0, 2, 60], [0, 2, 70], [0, 2, 80], [0, 3, 10], [0, 3, 20], [0, 3, 30], [0, 3, 40], [0, 3, 50], [0, 3, 60], [0, 3, 70], [0, 3, 80]]\n",
      "num of files 32\n",
      "samling strategy prob-based [0.0, 0.8]\n"
     ]
    }
   ],
   "source": [
    " from types import SimpleNamespace\n",
    "args = {\n",
    "    \"num_layers\": 48,\n",
    "    \"latent_dim\": 12,\n",
    "    \"experiment_folder\": \"/l/users/fathinah.izzati/coco-mulla-repo/expe\",\n",
    "    \"experiment_name\": \"experiment_1\",\n",
    "    \"prompt_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.prompt.txt\",\n",
    "    'sampling_strategy':'prob-based',\n",
    "    \"dataset\": '/l/users/fathinah.izzati/coco-mulla-repo/train.lst',\n",
    "    'learning_rate':0.1\n",
    "\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "experiment_folder = args.experiment_folder\n",
    "experiment_name = args.experiment_name\n",
    "if not os.path.exists(experiment_folder):\n",
    "    os.mkdir(experiment_folder)\n",
    "model_dir = os.path.join(experiment_folder, experiment_name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    " dataset, dataloader = get_dataset(\n",
    "        dataset_split=args.dataset,\n",
    "        sampling_strategy=args.sampling_strategy,\n",
    "        sampling_prob=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load....musicgen bk\n",
      "lm_bk, here\n",
      "cp_transformer.masked_embedding\n",
      "cp_transformer.pos_emb\n",
      "cp_transformer.gates\n",
      "cp_transformer.encodec_emb.weight\n",
      "cp_transformer.merge_linear.0.weight\n",
      "cp_transformer.merge_linear.1.weight\n",
      "cp_transformer.merge_linear.2.weight\n",
      "cp_transformer.merge_linear.3.weight\n",
      "cp_transformer.merge_linear.4.weight\n",
      "cp_transformer.merge_linear.5.weight\n",
      "cp_transformer.merge_linear.6.weight\n",
      "cp_transformer.merge_linear.7.weight\n",
      "cp_transformer.merge_linear.8.weight\n",
      "cp_transformer.merge_linear.9.weight\n",
      "cp_transformer.merge_linear.10.weight\n",
      "cp_transformer.merge_linear.11.weight\n",
      "cp_transformer.merge_linear.12.weight\n",
      "cp_transformer.merge_linear.13.weight\n",
      "cp_transformer.merge_linear.14.weight\n",
      "cp_transformer.merge_linear.15.weight\n",
      "cp_transformer.merge_linear.16.weight\n",
      "cp_transformer.merge_linear.17.weight\n",
      "cp_transformer.merge_linear.18.weight\n",
      "cp_transformer.merge_linear.19.weight\n",
      "cp_transformer.merge_linear.20.weight\n",
      "cp_transformer.merge_linear.21.weight\n",
      "cp_transformer.merge_linear.22.weight\n",
      "cp_transformer.merge_linear.23.weight\n",
      "cp_transformer.merge_linear.24.weight\n",
      "cp_transformer.merge_linear.25.weight\n",
      "cp_transformer.merge_linear.26.weight\n",
      "cp_transformer.merge_linear.27.weight\n",
      "cp_transformer.merge_linear.28.weight\n",
      "cp_transformer.merge_linear.29.weight\n",
      "cp_transformer.merge_linear.30.weight\n",
      "cp_transformer.merge_linear.31.weight\n",
      "cp_transformer.merge_linear.32.weight\n",
      "cp_transformer.merge_linear.33.weight\n",
      "cp_transformer.merge_linear.34.weight\n",
      "cp_transformer.merge_linear.35.weight\n",
      "cp_transformer.merge_linear.36.weight\n",
      "cp_transformer.merge_linear.37.weight\n",
      "cp_transformer.merge_linear.38.weight\n",
      "cp_transformer.merge_linear.39.weight\n",
      "cp_transformer.merge_linear.40.weight\n",
      "cp_transformer.merge_linear.41.weight\n",
      "cp_transformer.merge_linear.42.weight\n",
      "cp_transformer.merge_linear.43.weight\n",
      "cp_transformer.merge_linear.44.weight\n",
      "cp_transformer.merge_linear.45.weight\n",
      "cp_transformer.merge_linear.46.weight\n",
      "cp_transformer.merge_linear.47.weight\n",
      "cp_transformer.piano_roll_emb.0.weight\n",
      "cp_transformer.piano_roll_emb.1.weight\n",
      "cp_transformer.piano_roll_emb.2.weight\n",
      "cp_transformer.piano_roll_emb.3.weight\n",
      "cp_transformer.piano_roll_emb.4.weight\n",
      "cp_transformer.piano_roll_emb.5.weight\n",
      "cp_transformer.piano_roll_emb.6.weight\n",
      "cp_transformer.piano_roll_emb.7.weight\n",
      "cp_transformer.piano_roll_emb.8.weight\n",
      "cp_transformer.piano_roll_emb.9.weight\n",
      "cp_transformer.piano_roll_emb.10.weight\n",
      "cp_transformer.piano_roll_emb.11.weight\n",
      "cp_transformer.piano_roll_emb.12.weight\n",
      "cp_transformer.piano_roll_emb.13.weight\n",
      "cp_transformer.piano_roll_emb.14.weight\n",
      "cp_transformer.piano_roll_emb.15.weight\n",
      "cp_transformer.piano_roll_emb.16.weight\n",
      "cp_transformer.piano_roll_emb.17.weight\n",
      "cp_transformer.piano_roll_emb.18.weight\n",
      "cp_transformer.piano_roll_emb.19.weight\n",
      "cp_transformer.piano_roll_emb.20.weight\n",
      "cp_transformer.piano_roll_emb.21.weight\n",
      "cp_transformer.piano_roll_emb.22.weight\n",
      "cp_transformer.piano_roll_emb.23.weight\n",
      "cp_transformer.piano_roll_emb.24.weight\n",
      "cp_transformer.piano_roll_emb.25.weight\n",
      "cp_transformer.piano_roll_emb.26.weight\n",
      "cp_transformer.piano_roll_emb.27.weight\n",
      "cp_transformer.piano_roll_emb.28.weight\n",
      "cp_transformer.piano_roll_emb.29.weight\n",
      "cp_transformer.piano_roll_emb.30.weight\n",
      "cp_transformer.piano_roll_emb.31.weight\n",
      "cp_transformer.piano_roll_emb.32.weight\n",
      "cp_transformer.piano_roll_emb.33.weight\n",
      "cp_transformer.piano_roll_emb.34.weight\n",
      "cp_transformer.piano_roll_emb.35.weight\n",
      "cp_transformer.piano_roll_emb.36.weight\n",
      "cp_transformer.piano_roll_emb.37.weight\n",
      "cp_transformer.piano_roll_emb.38.weight\n",
      "cp_transformer.piano_roll_emb.39.weight\n",
      "cp_transformer.piano_roll_emb.40.weight\n",
      "cp_transformer.piano_roll_emb.41.weight\n",
      "cp_transformer.piano_roll_emb.42.weight\n",
      "cp_transformer.piano_roll_emb.43.weight\n",
      "cp_transformer.piano_roll_emb.44.weight\n",
      "cp_transformer.piano_roll_emb.45.weight\n",
      "cp_transformer.piano_roll_emb.46.weight\n",
      "cp_transformer.piano_roll_emb.47.weight\n",
      "trainable params: 107,700,400 || all params: 3,364,657,328 || trainable%: 3.200932205004622\n"
     ]
    }
   ],
   "source": [
    "model = CoCoMulla(TrainCfg.sample_sec, num_layers=args.num_layers, latent_dim=args.latent_dim).to(device)\n",
    "model.set_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 188.00 MiB. GPU 0 has a total capacty of 23.62 GiB of which 7.19 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 138.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 126\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset, dataloader, device, model_dir, learning_rate)\u001b[0m\n\u001b[1;32m    116\u001b[0m batch_1 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m: mix,\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrums\u001b[39m\u001b[38;5;124m\"\u001b[39m: drums,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m }\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# with autocast:\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m r_loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, mix\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m    129\u001b[0m grad_1, lr_1 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mstep(r_loss, model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/model.py:356\u001b[0m, in \u001b[0;36mCoCoMulla.forward\u001b[0;34m(self, seq, piano_roll, desc, chords, cond_mask, drums, num_samples, mode, max_n_frames, prompt_tokens)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq, piano_roll, desc, chords, cond_mask, drums,\n\u001b[1;32m    355\u001b[0m             num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_n_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, prompt_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 356\u001b[0m     embed_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcp_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mpiano_roll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpiano_roll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcond_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mchords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_n_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_n_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_model(seq, desc\u001b[38;5;241m=\u001b[39mdesc, embed_fn\u001b[38;5;241m=\u001b[39membed_fn,\n\u001b[1;32m    362\u001b[0m                           mode\u001b[38;5;241m=\u001b[39mmode, num_samples\u001b[38;5;241m=\u001b[39mnum_samples, total_gen_len\u001b[38;5;241m=\u001b[39mmax_n_frames,\n\u001b[1;32m    363\u001b[0m                           prompt_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/model.py:303\u001b[0m, in \u001b[0;36mCPTransformer.forward\u001b[0;34m(self, drums, chords, piano_roll, cond_mask, max_n_frames, mode, skip)\u001b[0m\n\u001b[1;32m    301\u001b[0m cond_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond_mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, cond, mask_embedding_per_layer)\n\u001b[1;32m    302\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_linear[i](cond_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, :T]\u001b[38;5;241m.\u001b[39mrepeat(B, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 303\u001b[0m q, k, v, o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    305\u001b[0m     outs\u001b[38;5;241m.\u001b[39mappend([[torch\u001b[38;5;241m.\u001b[39mcat([q, q], \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    306\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mcat([k, k], \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    307\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mcat([v, v], \u001b[38;5;241m0\u001b[39m)], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgates[i]])\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/model.py:212\u001b[0m, in \u001b[0;36mCPTransformerLayer.forward\u001b[0;34m(self, x, cond)\u001b[0m\n\u001b[1;32m    207\u001b[0m     q, k, v, o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(nx, nx, nx, emb_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    208\u001b[0m                                 attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m                                 key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    210\u001b[0m                                 need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_qkv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    211\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(o))\n\u001b[0;32m--> 212\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q, k, v, x\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/model.py:202\u001b[0m, in \u001b[0;36mCPTransformerLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 188.00 MiB. GPU 0 has a total capacty of 23.62 GiB of which 7.19 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 138.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(model, dataset, dataloader, device, model_dir, args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference step \n",
    "\n",
    "1. load  drums_rvq, midi, chord. drums rvq is obtained from wav and separate() function. then, they crop each of this control based on sample_sec (duration) and resolution\n",
    "2. load the condition mask \n",
    "3. wrap into a batch, multiplying each sample to 4 because there are four types of output, controlled by chord only, chord-midi, chord-drums, chords-drum-midi\n",
    "4. generate(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import spawn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from coco_mulla.utilities.trainer_utils import Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from coco_mulla.models import CoCoMulla\n",
    "\n",
    "device = \"cuda\"\n",
    "N_GPUS = 4\n",
    "\n",
    "## fixed\n",
    "from coco_mulla.data_loader.dataset_sampler import Dataset, collate_fn\n",
    "from config import TrainCfg\n",
    "\n",
    "def _get_free_port():\n",
    "    import socketserver\n",
    "    with socketserver.TCPServer(('localhost', 0), None) as s:\n",
    "        return s.server_address[1]\n",
    "\n",
    "\n",
    "def get_dataset(rid, dataset_path, sampling_strategy, sampling_prob):\n",
    "\n",
    "    dataset = Dataset(\n",
    "        rid=rid,\n",
    "        path_lst=[dataset_path],\n",
    "        sampling_prob=sampling_prob,\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        cfg=TrainCfg)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=TrainCfg.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    return dataset, dataloader\n",
    "def train_dist(replica_id, replica_count, port, model_dir, args):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = str(port)\n",
    "    torch.distributed.init_process_group('nccl', rank=replica_id, world_size=replica_count)\n",
    "    device = torch.device('cuda', replica_id)\n",
    "    torch.cuda.set_device(device)\n",
    "    model = CoCoMulla(TrainCfg.sample_sec, num_layers=args.num_layers, latent_dim=args.latent_dim).to(device)\n",
    "    model.set_training()\n",
    "    model = DDP(model, [replica_id])\n",
    "    dataset, dataloader = get_dataset(rid=replica_id, dataset_split=args.dataset,\n",
    "                                      sampling_strategy=args.sampling_strategy,\n",
    "                                      sampling_prob=[args.sampling_prob_a, args.sampling_prob_b])\n",
    "\n",
    "    train(replica_id, model, dataset, dataloader, device, model_dir,\n",
    "          args.learning_rate)\n",
    "\n",
    "\n",
    "def loss_fn(outputs, y):\n",
    "    prob = outputs.logits\n",
    "    mask = outputs.mask\n",
    "    prob = prob[mask]\n",
    "    y = y[mask]\n",
    "    prob = prob.view(-1, 2048)\n",
    "    return nn.CrossEntropyLoss()(prob, y)\n",
    "def train(rank, model, dataset, dataloader, device, model_dir, learning_rate):\n",
    "    # optimizer and lr scheduler\n",
    "    num_steps = len(dataloader)\n",
    "    epochs = TrainCfg.epoch\n",
    "    rng = np.random.RandomState(569 + rank * 100)\n",
    "    if rank == 0:\n",
    "        writer = SummaryWriter(model_dir, flush_secs=20)\n",
    "\n",
    "    trainer = Trainer(params=model.parameters(), lr=learning_rate, num_epochs=epochs, num_steps=num_steps)\n",
    "\n",
    "    model = model.to(device)\n",
    "    step = 0\n",
    "    for e in range(0, epochs):\n",
    "        mean_loss = 0\n",
    "        n_element = 0\n",
    "        model.train()\n",
    "\n",
    "        dl = tqdm(dataloader, desc=f\"Epoch {e}\") if rank == 0 else dataloader\n",
    "        r = rng.randint(0, 233333)\n",
    "        dataset.reset_random_seed(r, e)\n",
    "        for i, batch in enumerate(dl):\n",
    "            desc = batch[\"desc\"]\n",
    "            mix = batch[\"mix\"].to(device).long()\n",
    "            rgb_emb = batch[\"rgb_emb\"].to(device).long()\n",
    "            cond_mask = batch[\"cond_mask\"].to(device).long()\n",
    "\n",
    "            batch_1 = {\n",
    "                 \"seq\": mix,\n",
    "                \"rgb_emb\":rgb_emb,\n",
    "                \"cond_mask\": cond_mask,\n",
    "                \"desc\": desc,\n",
    "\n",
    "            }\n",
    "            print('seq',batch_1['seq'].shape)\n",
    "            print('rgb_emb',batch_1['rgb_emb'].shape)\n",
    "            print('cond_mask',batch_1['cond_mask'].shape)\n",
    "            print('desc',batch_1['desc'])\n",
    "\n",
    "            \n",
    "            # with autocast:\n",
    "            outputs = model(**batch_1)\n",
    "            r_loss = loss_fn(outputs, mix.long())\n",
    "            grad_1, lr_1 = trainer.step(r_loss, model.parameters())\n",
    "\n",
    "            step += 1\n",
    "            n_element += 1\n",
    "            if rank == 0:\n",
    "                writer.add_scalar(\"r_loss\", r_loss.item(), step)\n",
    "                writer.add_scalar(\"grad_1\", grad_1, step)\n",
    "                writer.add_scalar(\"lr_1\", lr_1, step)\n",
    "\n",
    "            mean_loss += r_loss.item()\n",
    "\n",
    "        mean_loss = mean_loss / n_element\n",
    "        if rank == 0:\n",
    "            with torch.no_grad():\n",
    "                writer.add_scalar('train/mean_loss', mean_loss, step)\n",
    "                model.module.save_weights(os.path.join(model_dir, f\"diff_{e}_end.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /l/users/fathinah.izzati/datasets/test.lst\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/l/users/fathinah.izzati/datasets/test.lst'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset, dataloader \u001b[38;5;241m=\u001b[39m  \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/l/users/fathinah.izzati/datasets/test.lst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprob-based\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleNamespace\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m48\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m768\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     15\u001b[0m }\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(rid, dataset_path, sampling_strategy, sampling_prob)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset\u001b[39m(rid, dataset_path, sampling_strategy, sampling_prob):\n\u001b[0;32m---> 38\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_lst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrainCfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     46\u001b[0m         dataset,\n\u001b[1;32m     47\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mTrainCfg\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m         drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset, dataloader\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/data_loader/dataset_sampler.py:41\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, path_lst, cfg, rid, sampling_prob, sampling_strategy, inference)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(path_lst):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i,path)\n\u001b[0;32m---> 41\u001b[0m     data, data_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_sec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data, data_index)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/data_loader/dataset_sampler.py:7\u001b[0m, in \u001b[0;36mload_data_from_path\u001b[0;34m(path, idx, sec)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data_from_path\u001b[39m(path, idx, sec):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/l/users/fathinah.izzati/datasets/test.lst'"
     ]
    }
   ],
   "source": [
    "dataset, dataloader =  get_dataset(1, '/l/users/fathinah.izzati/datasets/test.lst', 'prob-based', None)\n",
    "\n",
    "from types import SimpleNamespace\n",
    "args = {\n",
    "    \"num_layers\": 48,\n",
    "    \"latent_dim\": 768,\n",
    "    \"experiment_folder\": \"/l/users/fathinah.izzati/coco-mulla-repo/expe\",\n",
    "    \"experiment_name\": \"experiment_1\",\n",
    "    \"prompt_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.prompt.txt\",\n",
    "    'sampling_strategy':'prob-based',\n",
    "    'sampling_prob_a':0.5,\n",
    "    'sampling_prob_b':0.5,\n",
    "    \"dataset\": None,\n",
    "    'learning_rate':0.1\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "model = CoCoMulla(TrainCfg.sample_sec, num_layers=args.num_layers, latent_dim=args.latent_dim).to(device)\n",
    "model.set_training()\n",
    "# model = DDP(model, [0])\\\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "train(1, model, dataset, dataloader, device,  '/l/users/fathinah.izzati/coco-mulla-repo/demo',args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
