{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import librosa\n",
    "\n",
    "from coco_mulla.models import CoCoMulla\n",
    "from coco_mulla.utilities import *\n",
    "from coco_mulla.utilities.encodec_utils import extract_rvq, save_rvq\n",
    "from coco_mulla.utilities.symbolic_utils import process_midi, process_chord\n",
    "\n",
    "from coco_mulla.utilities.sep_utils import separate\n",
    "from config import TrainCfg\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model_path, batch):\n",
    "    model = CoCoMulla(TrainCfg.sample_sec,\n",
    "                      num_layers=args.num_layers,\n",
    "                      latent_dim=args.latent_dim).to(device)\n",
    "    model.load_weights(model_path)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model(**batch)\n",
    "\n",
    "    return gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(xlen):\n",
    "    names = [\"chord-only\", \"chord-drums\", \"chord-midi\", \"chord-drums-midi\"]\n",
    "    mask = torch.zeros([4, 2, xlen]).to(device)\n",
    "    mask[1, 1] = 1\n",
    "    mask[2, 0] = 1\n",
    "    mask[3] += 1\n",
    "    return mask, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(audio_path, chord_path, midi_path, offset):\n",
    "    sr = TrainCfg.sample_rate\n",
    "    res = TrainCfg.frame_res\n",
    "    sample_sec = TrainCfg.sample_sec\n",
    "\n",
    "    wav, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
    "    wav = np2torch(wav).to(device)[None, None, ...]\n",
    "    wavs = separate(wav, sr)\n",
    "    drums_rvq = extract_rvq(wavs[\"drums\"], sr=sr)\n",
    "    chord, _ = process_chord(chord_path)\n",
    "    flatten_midi_path = midi_path + \".piano.mid\"\n",
    "    midi, _ = process_midi(midi_path)\n",
    "\n",
    "\n",
    "\n",
    "    chord = crop(chord[None, ...], \"chord\", sample_sec, res)\n",
    "    pad_chord = chord.sum(-1, keepdims=True) == 0\n",
    "    chord = np.concatenate([chord, pad_chord], -1)\n",
    "\n",
    "    midi = crop(midi[None, ...], \"midi\", sample_sec, res,offset=offset)\n",
    "    drums_rvq = crop(drums_rvq[None, ...], \"drums_rvq\", sample_sec, res, offset=offset)\n",
    "\n",
    "    chord = torch.from_numpy(chord).to(device).float()\n",
    "    midi = torch.from_numpy(midi).to(device).float()\n",
    "    drums_rvq = drums_rvq.to(device).long()\n",
    "\n",
    "    return drums_rvq, midi, chord\n",
    "\n",
    "\n",
    "def crop(x, mode, sample_sec, res, offset=0):\n",
    "    xlen = x.shape[1] if mode == \"chord\" or mode == \"midi\" else x.shape[-1]\n",
    "    sample_len = int(sample_sec * res) + 1\n",
    "    if xlen < sample_len:\n",
    "        if mode == \"chord\" or mode == \"midi\":\n",
    "            x = np.pad(x, ((0, 0), (0, sample_len - xlen), (0, 0)))\n",
    "        else:\n",
    "            x = F.pad(x, (0, sample_len - xlen), \"constant\", 0)\n",
    "        return x\n",
    "\n",
    "    st = offset * res\n",
    "    ed = int((offset + sample_sec) * res) + 1\n",
    "    if mode == \"chord\" or mode == \"midi\":\n",
    "        assert x.shape[1] > st\n",
    "        return x[:, st: ed]\n",
    "    assert x.shape[2] > ed\n",
    "    return x[:, :, st: ed]\n",
    "\n",
    "\n",
    "def save_pred(output_folder, tags, pred):\n",
    "    mkdir(output_folder)\n",
    "    output_list = [os.path.join(output_folder, tag) for tag in tags]\n",
    "    save_rvq(output_list=output_list, tokens=pred)\n",
    "\n",
    "\n",
    "def wrap_batch(drums_rvq, midi, chord, cond_mask, prompt):\n",
    "    num_samples = len(cond_mask)\n",
    "    midi = midi.repeat(num_samples, 1, 1)\n",
    "    chord = chord.repeat(num_samples, 1, 1)\n",
    "    drums_rvq = drums_rvq.repeat(num_samples, 1, 1)\n",
    "    prompt = [prompt] * num_samples\n",
    "    batch = {\n",
    "        \"seq\": None,\n",
    "        \"desc\": prompt,\n",
    "        \"chords\": chord,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"cond_mask\": cond_mask,\n",
    "        \"drums\": drums_rvq,\n",
    "        \"piano_roll\": midi,\n",
    "        \"mode\": \"inference\",\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def inference(args):\n",
    "    drums_rvq, midi, chord = load_data(audio_path=args.audio_path,\n",
    "                                       chord_path=args.chord_path,\n",
    "                                       midi_path=args.midi_path,\n",
    "                                       offset=args.offset)\n",
    "    print(drums_rvq)\n",
    "    print('================================================================')\n",
    "    print(midi)\n",
    "    print('================================================================')\n",
    "    print(chord)\n",
    "    \n",
    "    cond_mask, names = generate_mask(drums_rvq.shape[-1])\n",
    "    batch = wrap_batch(drums_rvq, midi, chord, cond_mask, read_lst(args.prompt_path)[0])\n",
    "    pred = generate(model_path=args.model_path,\n",
    "                    batch=batch)\n",
    "    save_pred(output_folder=args.output_folder,\n",
    "              tags=names,\n",
    "              pred=pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = {\n",
    "    \"num_layers\": 48,\n",
    "    \"latent_dim\": 12,\n",
    "    \"output_folder\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/output\",\n",
    "    \"model_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/diff_9_end.pth\",\n",
    "    \"audio_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.flac\",\n",
    "    \"prompt_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.prompt.txt\",\n",
    "    \"chord_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.flac.chord.lab\",\n",
    "    \"midi_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/let_it_be.mid.piano.mid\",\n",
    "    \"drums_path\": None,\n",
    "    \"offset\": 0\n",
    "}\n",
    "args = SimpleNamespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drums_rvq, midi, chord = load_data(audio_path=args.audio_path,\n",
    "                                       chord_path=args.chord_path,\n",
    "                                       midi_path=args.midi_path,\n",
    "                                       offset=args.offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 1001]),\n",
       " torch.Size([1, 1001, 128]),\n",
       " torch.Size([1, 1001, 37]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drums_rvq.shape,midi.shape, chord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_mask, names = generate_mask(drums_rvq.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(cond_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi = midi.repeat(num_samples, 1, 1)\n",
    "chord = chord.repeat(num_samples, 1, 1)\n",
    "drums_rvq = drums_rvq.repeat(num_samples, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inference(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import spawn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from coco_mulla.utilities.trainer_utils import Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from config import TrainCfg\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from coco_mulla.data_loader.cc_dataset_sampler import Dataset, collate_fn\n",
    "from coco_mulla.models import CoCoMulla\n",
    "\n",
    "device = \"cuda\"\n",
    "N_GPUS = 4\n",
    "\n",
    "\n",
    "def _get_free_port():\n",
    "    import socketserver\n",
    "    with socketserver.TCPServer(('localhost', 0), None) as s:\n",
    "        return s.server_address[1]\n",
    "\n",
    "\n",
    "def get_dataset(rid, dataset_split, sampling_strategy, sampling_prob):\n",
    "\n",
    "    file_lst = [\"data/text/musdb18_full.lst\",\n",
    "                \"data/text/closed_dataset_fm_full.lst\"]\n",
    "    splits = [\n",
    "        [1],\n",
    "        [0],\n",
    "        [0, 1],\n",
    "    ]\n",
    "    dataset = Dataset(\n",
    "        rid=rid,\n",
    "        path_lst=[file_lst[i] for i in splits[dataset_split]],\n",
    "        sampling_prob=sampling_prob,\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        cfg=TrainCfg)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=TrainCfg.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        sampler=DistributedSampler(dataset),\n",
    "        pin_memory=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "def train_dist(replica_id, replica_count, port, model_dir, args):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = str(port)\n",
    "    torch.distributed.init_process_group('nccl', rank=replica_id, world_size=replica_count)\n",
    "    device = torch.device('cuda', replica_id)\n",
    "    torch.cuda.set_device(device)\n",
    "    model = CoCoMulla(TrainCfg.sample_sec, num_layers=args.num_layers, latent_dim=args.latent_dim).to(device)\n",
    "    model.set_training()\n",
    "    model = DDP(model, [replica_id])\n",
    "    dataset, dataloader = get_dataset(rid=replica_id, dataset_split=args.dataset,\n",
    "                                      sampling_strategy=args.sampling_strategy,\n",
    "                                      sampling_prob=[args.sampling_prob_a, args.sampling_prob_b])\n",
    "\n",
    "    train(replica_id, model, dataset, dataloader, device, model_dir,\n",
    "          args.learning_rate)\n",
    "\n",
    "\n",
    "def loss_fn(outputs, y):\n",
    "    prob = outputs.logits\n",
    "    mask = outputs.mask\n",
    "    prob = prob[mask]\n",
    "    y = y[mask]\n",
    "    prob = prob.view(-1, 2048)\n",
    "    return nn.CrossEntropyLoss()(prob, y)\n",
    "\n",
    "\n",
    "def train(rank, model, dataset, dataloader, device, model_dir, learning_rate):\n",
    "    # optimizer and lr scheduler\n",
    "    num_steps = len(dataloader)\n",
    "    epochs = TrainCfg.epoch\n",
    "    rng = np.random.RandomState(569 + rank * 100)\n",
    "    if rank == 0:\n",
    "        writer = SummaryWriter(model_dir, flush_secs=20)\n",
    "\n",
    "    trainer = Trainer(params=model.parameters(), lr=learning_rate, num_epochs=epochs, num_steps=num_steps)\n",
    "\n",
    "    model = model.to(device)\n",
    "    step = 0\n",
    "    for e in range(0, epochs):\n",
    "        mean_loss = 0\n",
    "        n_element = 0\n",
    "        model.train()\n",
    "\n",
    "        dl = tqdm(dataloader, desc=f\"Epoch {e}\") if rank == 0 else dataloader\n",
    "        r = rng.randint(0, 233333)\n",
    "        dataset.reset_random_seed(r, e)\n",
    "        for i, batch in enumerate(dl):\n",
    "            desc = batch[\"desc\"]\n",
    "            mix = batch[\"mix\"].to(device).long()\n",
    "            drums = batch[\"drums\"].to(device).long()\n",
    "            chords = batch[\"chords\"].to(device).float()\n",
    "            piano_roll = batch[\"piano_roll\"].to(device).float()\n",
    "            cond_mask = batch[\"cond_mask\"].to(device).long()\n",
    "\n",
    "            batch_1 = {\n",
    "                \"seq\": mix,\n",
    "                \"drums\": drums,\n",
    "                \"chords\": chords,\n",
    "                \"piano_roll\": piano_roll,\n",
    "                \"cond_mask\": cond_mask,\n",
    "                \"desc\": desc,\n",
    "\n",
    "            }\n",
    "            # with autocast:\n",
    "            outputs = model(**batch_1)\n",
    "            r_loss = loss_fn(outputs, mix.long())\n",
    "\n",
    "            grad_1, lr_1 = trainer.step(r_loss, model.parameters())\n",
    "\n",
    "            step += 1\n",
    "            n_element += 1\n",
    "            if rank == 0:\n",
    "                writer.add_scalar(\"r_loss\", r_loss.item(), step)\n",
    "                writer.add_scalar(\"grad_1\", grad_1, step)\n",
    "                writer.add_scalar(\"lr_1\", lr_1, step)\n",
    "\n",
    "            mean_loss += r_loss.item()\n",
    "\n",
    "        mean_loss = mean_loss / n_element\n",
    "        if rank == 0:\n",
    "            with torch.no_grad():\n",
    "                writer.add_scalar('train/mean_loss', mean_loss, step)\n",
    "                model.module.save_weights(os.path.join(model_dir, f\"diff_{e}_end.pth\"))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    experiment_folder = args.experiment_folder\n",
    "    experiment_name = args.experiment_name\n",
    "\n",
    "    if not os.path.exists(experiment_folder):\n",
    "        os.mkdir(experiment_folder)\n",
    "    model_dir = os.path.join(experiment_folder, experiment_name)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    world_size = N_GPUS\n",
    "    port = _get_free_port()\n",
    "    spawn(train_dist, args=(world_size, port, model_dir, args), nprocs=world_size, join=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--experiment_folder', type=str)\n",
    "    parser.add_argument('-n', '--experiment_name', type=str)\n",
    "    parser.add_argument('-l', '--num_layers', type=int)\n",
    "    parser.add_argument('-t', '--text_path', type=str, default=None)\n",
    "    parser.add_argument('-r', '--latent_dim', type=int)\n",
    "    parser.add_argument('-lr', '--learning_rate', type=float)\n",
    "    parser.add_argument('-s', '--sampling_strategy', type=str)\n",
    "    parser.add_argument('-a', '--sampling_prob_a', type=float, default=0.)\n",
    "    parser.add_argument('-b', '--sampling_prob_b', type=float, default=0.)\n",
    "    parser.add_argument('-ds', '--dataset', type=int, default=0)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cocomulla's repository is tructured into three parts:\n",
    "1. Input preparation \n",
    "This part includes loading the audio file, converting it to RVQ, processing the chord labels, and loading the MIDI file.\n",
    "2. COCO-Mulla model\n",
    "The COCO-Mulla model is implemented in the `coco_mulla/models/coco_mulla.py` file. This model consists of three main components: the encoder, the decoder, and the conditioning module. The encoder takes the RVQ representation of the drums and chords, and the conditioning module combines them to produce the input to the decoder. The decoder then generates the piano roll and drum samples.\n",
    "3. Inference\n",
    "The inference part is implemented in the `coco_mulla/inference.py` file. This script takes a pre-trained COCO-Mulla model(from above step) and a MIDI file as input, and generates a piano roll and drum sample.\n",
    "The provided code snippet is a part of the training process. The `train_dist` function is responsible for initializing the distributed training environment, loading the dataset, and training the COCO-Mulla model. The `loss_fn` function calculates the cross-entropy loss between the predicted drum samples and the ground truth drum samples. The `train` function is the main training loop, which iterates over the dataset, performs forward and backward passes, and logs the training progress using\n",
    "\n",
    "There are four scenarios:\n",
    "1. if conditioned by the chord only\n",
    "2. if conditioned by chord and midi\n",
    "3. if conditioned by chord and drums\n",
    "4. if conditioned by chord, midi, and drums\n",
    "\n",
    "Each of the output has piano roll and drum sample.\n",
    "\n",
    "I'm lookinh to change the conditioned input to video with a different encoder ( video encoder )\n",
    "In order to do so, I need to train the RVQ module with video input data.\n",
    "question: how to train the RVQ module for video?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Motion encoder\n",
    "import yaml\n",
    "import importlib\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n",
    "\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if \"target\" not in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vaura.models.modules.feature_extractors.avclip.motionformer.MotionFormer'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the YAML file\n",
    "with open(\"/l/users/fathinah.izzati/coco-mulla-repo/vaura/logs/24-08-01T08-34-26/vgg-9cb-viscond-avclip_delayed-llama-ib_03/hparams.yaml\", \"r\") as file:\n",
    "    hparams = yaml.safe_load(file)\n",
    "\n",
    "# Extract the feature_extractor_config\n",
    "feature_extractor_config = hparams['feature_extractor_config']\n",
    "feature_extractor_config['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MotionFormer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (patch_embed_3d): PatchEmbed3D(\n",
       "    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.018)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (2): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.036)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (3): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.055)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (4): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.073)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (5): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.091)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (6): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.109)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (7): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.127)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (8): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.145)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (9): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.164)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (10): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.182)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (11): DividedSpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): DividedAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.200)\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head_drop): Identity()\n",
       "  (head): Identity()\n",
       "  (spatial_attn_agg): SpatialTransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.0, inplace=False)\n",
       "    (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    (activation): GELU(approximate='none')\n",
       "  )\n",
       "  (temp_attn_agg): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_feature_extractor = instantiate_from_config(feature_extractor_config)\n",
    "visual_feature_extractor.eval()\n",
    "visual_feature_extractor.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_bridge = instantiate_from_config({'target': 'torch.nn.Identity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "MODEL_MAX_DURATION = 2.56  # do not modify\n",
    "DURATION = 2.56  # n * 0.64s\n",
    "STRIDE = 1.28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "dl_cfg = OmegaConf.load(\"./vaura/data/demo/dataloader_config.yaml\")\n",
    "dl_cfg[\"sample_duration\"] = DURATION\n",
    "OmegaConf.resolve(dl_cfg)  # resolve durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaura.utils.train_utils import get_datamodule_from_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/l/users/fathinah.izzati/coco-mulla-repo/vaura/models/data/motionformer_gen_dataset.py:18: UserWarning: File None does not exist. Sampled start points will be 0.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "datamodule = get_datamodule_from_type(\"motionformer_gen\", dl_cfg)\n",
    "datamodule.setup(\"test\")\n",
    "dataloader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1, 'num_workers': 0, 'path_to_metadata': 'vaura/data/demo', 'gen_videos_filepath': None, 'assert_fps': False, 'crop': False, 'partition_video_to_clips': True, 'sample_duration': 2.56, 'audio_transforms_test': [{'target': 'vaura.models.data.transforms.audio_transforms.AudioStereoToMono', 'params': {'keepdim': True}}, {'target': 'vaura.models.data.transforms.audio_transforms.AudioResample', 'params': {'target_sr': 44100, 'clip_duration': 2.56}}, {'target': 'vaura.models.data.transforms.audio_transforms.AudioTrim', 'params': {'duration': 2.56, 'sr': 44100}}], 'video_transforms_test': [{'target': 'vaura.models.data.transforms.video_transforms.Permute', 'params': {'permutation': [0, 3, 1, 2]}}, {'target': 'vaura.models.data.transforms.video_transforms.UniformTemporalSubsample', 'params': {'target_fps': 25, 'clip_duration': 2.56}}, {'target': 'vaura.models.data.transforms.video_transforms.Permute', 'params': {'permutation': [0, 2, 3, 1]}}, {'target': 'torchvision.transforms.v2.Resize', 'params': {'size': 256, 'antialias': True}}, {'target': 'torchvision.transforms.v2.CenterCrop', 'params': {'size': [224, 224]}}, {'target': 'vaura.models.data.transforms.video_transforms.ToFloat32DType'}, {'target': 'torchvision.transforms.v2.Normalize', 'params': {'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5]}}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve generation parameters\n",
    "MODEL_MAX_DURATION = 2.56  # do not modify\n",
    "COMPRESSION_MODEL_FRAME_RATE = 86  # do not modify\n",
    "\n",
    "\n",
    "total_gen_len = int(DURATION * COMPRESSION_MODEL_FRAME_RATE)\n",
    "stride_tokens = int(STRIDE * COMPRESSION_MODEL_FRAME_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x15327bcb5a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/xK-7W3ZPd3o_94000_104000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video xK-7W3ZPd3o_94000_104000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/76UZQRJq028_181000_191000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video 76UZQRJq028_181000_191000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/xK-7W3ZPd3o_94000_104000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video xK-7W3ZPd3o_94000_104000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/xK-7W3ZPd3o_94000_104000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video xK-7W3ZPd3o_94000_104000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/Vi7kQhNcaOs_114000_124000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video Vi7kQhNcaOs_114000_124000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/xK-7W3ZPd3o_94000_104000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video xK-7W3ZPd3o_94000_104000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/Vi7kQhNcaOs_114000_124000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video Vi7kQhNcaOs_114000_124000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/Vi7kQhNcaOs_114000_124000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video Vi7kQhNcaOs_114000_124000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/xK-7W3ZPd3o_94000_104000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video xK-7W3ZPd3o_94000_104000.mp4 could not be loaded correctly. Trying another one.\n",
      "ERROR:vaura.models.data.video_dataset:[Errno 2] No such file or directory: 'data/demo/76UZQRJq028_181000_191000.mp4'\n",
      "WARNING:vaura.models.data.video_dataset:Video 76UZQRJq028_181000_191000.mp4 could not be loaded correctly. Trying another one.\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Video could not be loaded correctly. Tried 10 times.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvis_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvisual_feature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/vaura/models/data/motionformer_dataset.py:81\u001b[0m, in \u001b[0;36mMotionFormerDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/vaura/models/data/vjepa_dataset.py:138\u001b[0m, in \u001b[0;36mVJEPADataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fps:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    141\u001b[0m             item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjepa_fps\n\u001b[1;32m    142\u001b[0m         ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo FPS is not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjepa_fps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/vaura/models/data/video_dataset.py:159\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the item at the given index.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_video_loading_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/vaura/models/data/video_dataset.py:209\u001b[0m, in \u001b[0;36mVideoDataset._video_loading_handler\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    206\u001b[0m         load_attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loaded_video:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo could not be loaded correctly. Tried \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_load_attempts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop:\n\u001b[1;32m    214\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m rgb[: \u001b[38;5;28mint\u001b[39m(meta\u001b[38;5;241m.\u001b[39mvideo_fps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_duration), \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Video could not be loaded correctly. Tried 10 times."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for sample in tqdm(dataloader):\n",
    "\n",
    "    frames = sample[\"frames\"]\n",
    "    vis_feats, _ = visual_feature_extractor(frames)\n",
    "    B, S, Tv, D = vis_feats.shape\n",
    "    vis_feats = vis_feats.reshape(B, S * Tv, D)\n",
    "    vis_feats = vis_feats.detach()\n",
    "    vis_feats = visual_bridge(vis_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocomulla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
